{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOl1y4PUrA4e",
        "outputId": "2c50a380-671e-4a04-bcc6-0184570cc6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tqdm nltk rapidfuzz\n",
        "\n",
        "import os, json, re, random\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from openai import OpenAI\n",
        "from rapidfuzz import fuzz\n",
        "# ==== 2Ô∏è‚É£ Inspect your crawled JSONL file ====\n",
        "import json\n",
        "import itertools\n",
        "\n",
        "# ==== 1. Setup ====\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-C6WGSP_rjv9BBrT-PZf86g\"\n",
        "client = OpenAI()\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/pittsburgh_cmu_knowledge_base.jsonl\"\n",
        "output_verified = \"/content/drive/MyDrive/synthetic_qa_verified.jsonl\"\n",
        "log_path = \"qa_reject_log.jsonl\"\n",
        "\n",
        "# Check file size and preview first few lines\n",
        "!ls -lh \"$input_path\"\n",
        "print(\"\\nüîç Previewing first few lines:\\n\")\n",
        "\n",
        "with open(input_path, 'r', encoding='utf8') as f:\n",
        "    for line in itertools.islice(f, 5):\n",
        "        print(line.strip()[:400], \"\\n---\")  # print first 400 chars per entry\n",
        "\n",
        "# ==== 3Ô∏è‚É£ Validate structure ====\n",
        "required_keys = {\"url\", \"title\", \"content\"}\n",
        "n_ok, n_bad = 0, 0\n",
        "bad_examples = []\n",
        "\n",
        "with open(input_path, 'r', encoding='utf8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            if not required_keys.issubset(data.keys()):\n",
        "                n_bad += 1\n",
        "                bad_examples.append({\"line\": i, \"missing\": list(required_keys - data.keys())})\n",
        "            else:\n",
        "                n_ok += 1\n",
        "        except json.JSONDecodeError:\n",
        "            n_bad += 1\n",
        "            bad_examples.append({\"line\": i, \"error\": \"Invalid JSON\"})\n",
        "# Open the file and read the first line\n",
        "with open(input_path, \"r\") as f:\n",
        "    first_line = f.readline()\n",
        "\n",
        "# Parse the line as JSON\n",
        "data = json.loads(first_line)\n",
        "\n",
        "# Print all keys\n",
        "print(\"Keys in this row:\")\n",
        "print(list(data.keys()))\n",
        "\n",
        "# (Optional) inspect one key-value pair\n",
        "for k, v in data.items():\n",
        "    print(f\"{k}: {str(v)[:100]}...\")  # preview first 100 chars\n",
        "\n",
        "print(f\"\\n‚úÖ Valid entries: {n_ok}\")\n",
        "print(f\"‚ö†Ô∏è  Problematic entries: {n_bad}\")\n",
        "\n",
        "if n_bad:\n",
        "    print(\"\\nSample issues:\")\n",
        "    for b in bad_examples[:5]:\n",
        "        print(b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHMxjUkwslX9",
        "outputId": "9429b1b3-20e3-44e1-a3d4-2c46193f15c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "-rw------- 1 root root 7.4M Oct 11 05:44 /content/drive/MyDrive/pittsburgh_cmu_knowledge_base.jsonl\n",
            "\n",
            "üîç Previewing first few lines:\n",
            "\n",
            "{\"url\": \"https://www.cmu.edu/about/\", \"title\": \"About Carnegie Mellon | Carnegie Mellon University\", \"content\": \"# About Carnegie Mellon\\n\\n## Don‚Äôt Envision the Future, Invent It Every Day\\n\\nAt Carnegie Mellon University, academic and research excellence isn't just what we strive for ‚Äî it's the uncompromising standard. Our entrepreneurial spirit means no pursuit is too complex, and no question i \n",
            "---\n",
            "{\"url\": \"https://www.cmu.edu/about/traditions\", \"title\": \"Traditions | Carnegie Mellon University\", \"content\": \"# Traditions\\n\\n## Connected to the Past, Shaping the Future\\n\\nAt Carnegie Mellon University (CMU), traditions are more than just events ‚Äî they're a celebration of our bold and collaborative community. Some commemorate the Scottish heritage of our founder, Andrew Carnegie, while others  \n",
            "---\n",
            "{\"url\": \"https://www.cmu.edu/about/vision-mission-values\", \"title\": \"Vision, Mission and Values | Carnegie Mellon University\", \"content\": \"# Vision, Mission and Values\\n\\n## Empowering the Next Generation of World-Changers\\n\\nCarnegie Mellon University‚Äôs vision, mission and values guide us in providing an exceptional educational experience and creating a diverse community of creators, inventors an \n",
            "---\n",
            "{\"url\": \"https://www.cmu.edu/about/history\", \"title\": \"History | Carnegie Mellon University\", \"content\": \"# History\\n\\n## Founder Spotlight\\n\\nA self-educated \\\"working boy,\\\" Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He  \n",
            "---\n",
            "{\"url\": \"https://www.cmu.edu/about/open-searches/avp-executive-director-swartz-center/search-announcement\", \"title\": \"Search Announcement | Carnegie Mellon University\", \"content\": \"# Search Announcement\\n\\n_August 4, 2025_\\n\\nDear Members of the Carnegie Mellon Community:\\n\\nI am pleased to officially announce a **global search for a new associate vice president for entrepreneurship and executive  \n",
            "---\n",
            "Keys in this row:\n",
            "['url', 'title', 'content', 'source_category', 'source_root_url', 'format', 'is_pdf', 'scraped_at', 'metadata']\n",
            "url: https://www.cmu.edu/about/...\n",
            "title: About Carnegie Mellon | Carnegie Mellon University...\n",
            "content: # About Carnegie Mellon\n",
            "\n",
            "## Don‚Äôt Envision the Future, Invent It Every Day\n",
            "\n",
            "At Carnegie Mellon Unive...\n",
            "source_category: general_info...\n",
            "source_root_url: https://www.cmu.edu/about/...\n",
            "format: HTML...\n",
            "is_pdf: False...\n",
            "scraped_at: 2025-10-05T03:06:40.689632...\n",
            "metadata: {'original_content_length': 5942, 'cleaned_content_length': 4473, 'word_count': 486, 'char_count': 4...\n",
            "\n",
            "‚úÖ Valid entries: 530\n",
            "‚ö†Ô∏è  Problematic entries: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDrSFE-zqio4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2630c2b0-1671-4b10-c7af-07d6e476ac7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Loaded 0 docs. Sampling 0 for generation.\n",
            "‚ö†Ô∏è No documents loaded. Check your input path or filtering conditions.\n",
            "‚úÖ Verified 0 of 0 QA pairs.\n",
            "‚ö†Ô∏è No verified file found ‚Äî possibly no valid QA pairs generated.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# =====================================================\n",
        "# ‚úÖ Pittsburgh / CMU QA Generator ‚Äì Fixed Version\n",
        "# =====================================================\n",
        "\n",
        "import os, json, re, random\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from openai import OpenAI\n",
        "from rapidfuzz import fuzz\n",
        "\n",
        "# ==== 1Ô∏è‚É£ Setup ====\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-C6WGSP_rjv9BBrT-PZf86g\"\n",
        "client = OpenAI()\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/pittsburgh_cmu_knowledge_base.jsonl\"\n",
        "output_verified = \"/content/drive/MyDrive/synthetic_qa_verified.jsonl\"\n",
        "log_path = \"/content/drive/MyDrive/qa_reject_log.jsonl\"\n",
        "\n",
        "# ‚úÖ Make sure output files exist\n",
        "for p in [output_verified, log_path]:\n",
        "    if not os.path.exists(p):\n",
        "        open(p, \"w\").close()\n",
        "\n",
        "# ==== 2Ô∏è‚É£ Utilities ====\n",
        "def clean_text(t):\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    t = re.sub(r\"(?i)(cookie|subscribe|login|privacy|footer)\", \"\", t)\n",
        "    return t.strip()\n",
        "\n",
        "def chunk_text(text, max_len=250):\n",
        "    sents = sent_tokenize(text)\n",
        "    chunks, cur, cur_len = [], [], 0\n",
        "    for s in sents:\n",
        "        words = s.split()\n",
        "        if cur_len + len(words) > max_len:\n",
        "            chunks.append(\" \".join(cur))\n",
        "            cur = cur[-2:]\n",
        "            cur_len = len(\" \".join(cur).split())\n",
        "        cur.extend(words)\n",
        "        cur_len += len(words)\n",
        "    if cur:\n",
        "        chunks.append(\" \".join(cur))\n",
        "    return chunks\n",
        "\n",
        "def parse_jsonl_block(raw_text):\n",
        "    \"\"\"Extract JSON objects from model output.\"\"\"\n",
        "    matches = re.findall(r'\\{.*?\\}', raw_text, flags=re.S)\n",
        "    results = []\n",
        "    for m in matches:\n",
        "        try:\n",
        "            results.append(json.loads(m))\n",
        "        except:\n",
        "            pass\n",
        "    return results\n",
        "\n",
        "# ==== 3Ô∏è‚É£ Load and preprocess documents ====\n",
        "docs = []\n",
        "with open(input_path, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            d = json.loads(line)\n",
        "            text = clean_text(d.get(\"content\", \"\"))\n",
        "            # Loosened filter for better coverage\n",
        "            if 100 < len(text) < 4000:\n",
        "                chunks = chunk_text(text)\n",
        "                for c in chunks:\n",
        "                    docs.append({\"url\": d.get(\"url\"), \"title\": d.get(\"title\"), \"text\": c})\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "random.shuffle(docs)\n",
        "sampled = docs[:500]   # adjust for time/budget\n",
        "print(f\"üìÑ Loaded {len(docs)} docs. Sampling {len(sampled)} for generation.\")\n",
        "\n",
        "# ==== 4Ô∏è‚É£ Generate QA pairs ====\n",
        "def generate_qa(text):\n",
        "    prompt = f\"\"\"\n",
        "You are a meticulous human annotator creating high-quality, factual question‚Äìanswer (QA) pairs\n",
        "for a test dataset about Pittsburgh and Carnegie Mellon University.\n",
        "\n",
        "Given the text below, extract 2‚Äì3 diverse, accurate QA pairs that can be verified *verbatim*\n",
        "from the text itself.\n",
        "\n",
        "Annotation requirements:\n",
        "- Each question must be short, specific, factual, and directly answerable from the passage.\n",
        "- Use factual forms: who / what / when / where / how.\n",
        "- Each answer must be copied *exactly* from the text, not paraphrased.\n",
        "- If a question has multiple valid answers, list all of them separated by a semicolon (;).\n",
        "- Cover diverse fact types when possible (people, places, organizations, events, dates, etc.).\n",
        "- If the text includes multiple time frames, entities, or organizations, make at least one question about each.\n",
        "- Include at least one question about subtle or less obvious details if present (for coverage of edge cases).\n",
        "- Prioritize questions relevant to Pittsburgh or Carnegie Mellon University when applicable.\n",
        "- Avoid vague, opinion-based, or unanswerable questions.\n",
        "- Do not reference ‚Äúthe text above‚Äù or ‚Äúthe passage.‚Äù\n",
        "- Keep answers under 20 words when possible.\n",
        "- Do NOT guess, summarize, or infer; only extract facts explicitly stated in the text.\n",
        "- If a fact is not explicitly stated in the text, skip it ‚Äî do NOT assume or invent.\n",
        "- If fewer than two valid QA pairs can be found, output only those that meet all requirements.\n",
        "- Maintain consistent style for quality.\n",
        "- Output in JSONL format, one line per QA pair:\n",
        "  {{\"question\": \"...\", \"answer\": \"...\"}}\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\"\"\"\n",
        "    try:\n",
        "        res = client.chat.completions.create(\n",
        "            model=\"gpt-5\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        return res.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(\"Error during generation:\", e)\n",
        "        return None\n",
        "\n",
        "# ==== 5Ô∏è‚É£ Verification helper ====\n",
        "def is_answer_in_text(answer, text):\n",
        "    \"\"\"Return True if answer appears in text (exact or fuzzy).\"\"\"\n",
        "    if not answer or not text:\n",
        "        return False\n",
        "    answer_low, text_low = answer.lower(), text.lower()\n",
        "    if answer_low in text_low:\n",
        "        return True\n",
        "    return fuzz.partial_ratio(answer_low, text_low) > 85\n",
        "\n",
        "# ==== 6Ô∏è‚É£ Main loop ====\n",
        "verified_count, total_generated = 0, 0\n",
        "\n",
        "if not sampled:\n",
        "    print(\"‚ö†Ô∏è No documents loaded. Check your input path or filtering conditions.\")\n",
        "else:\n",
        "    for d in tqdm(sampled, desc=\"Generating QA pairs\"):\n",
        "        qa_text = generate_qa(d[\"text\"])\n",
        "        if not qa_text:\n",
        "            continue\n",
        "\n",
        "        qa_pairs = parse_jsonl_block(qa_text)\n",
        "        for qa in qa_pairs:\n",
        "            total_generated += 1\n",
        "            if is_answer_in_text(qa.get(\"answer\", \"\"), d[\"text\"]):\n",
        "                qa[\"source_url\"] = d[\"url\"]\n",
        "                qa[\"source_title\"] = d[\"title\"]\n",
        "                with open(output_verified, \"a\", encoding=\"utf8\") as f:\n",
        "                    f.write(json.dumps(qa, ensure_ascii=False) + \"\\n\")\n",
        "                verified_count += 1\n",
        "            else:\n",
        "                with open(log_path, \"a\", encoding=\"utf8\") as f:\n",
        "                    f.write(json.dumps({\"rejected\": qa, \"source_excerpt\": d[\"text\"][:400]}) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Verified {verified_count} of {total_generated} QA pairs.\")\n",
        "\n",
        "# ==== 7Ô∏è‚É£ Post-processing cleanup ====\n",
        "if not os.path.exists(output_verified) or os.stat(output_verified).st_size == 0:\n",
        "    print(\"‚ö†Ô∏è No verified file found ‚Äî possibly no valid QA pairs generated.\")\n",
        "else:\n",
        "    qa_pairs = [json.loads(l) for l in open(output_verified, \"r\", encoding=\"utf8\") if l.strip()]\n",
        "\n",
        "    filtered = [\n",
        "        qa for qa in qa_pairs\n",
        "        if 5 < len(qa[\"question\"]) < 120 and 1 < len(qa[\"answer\"]) < 60\n",
        "    ]\n",
        "    json.dump(filtered, open(\"/content/drive/MyDrive/filtered_qa.json\", \"w\"), indent=2)\n",
        "    print(f\"‚ú® Final cleaned set: {len(filtered)} verified & length-filtered QA pairs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load your verified + filtered data\n",
        "with open(\"/content/drive/MyDrive/filtered_qa.json\") as f:\n",
        "    qa_pairs = json.load(f)\n",
        "\n",
        "# Create numbered mapping\n",
        "questions = []\n",
        "ref_answers = {}\n",
        "for i, qa in enumerate(qa_pairs, start=1):\n",
        "    questions.append(qa[\"question\"])\n",
        "    ref_answers[str(i)] = qa[\"answer\"]\n",
        "\n",
        "# Save to files\n",
        "with open(\"/content/drive/MyDrive/questions.txt\", \"w\") as fq:\n",
        "    fq.write(\"\\n\".join(questions))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/reference_answers.json\", \"w\") as fa:\n",
        "    json.dump(ref_answers, fa, indent=2)\n",
        "\n",
        "print(f\"‚úÖ Saved {len(questions)} training questions and answers in assignment format.\")\n"
      ],
      "metadata": {
        "id": "grqoBpMmvW5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2b414vqvrD2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}